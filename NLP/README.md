# NLP-анализ объявлений о наборе в рабочие дома

## Назначение

Скрипт `nlp_workhouse_analysis.py` выполняет комплексный текстовый анализ объявлений о наборе в так называемые «рабочие дома» — организации, предлагающие трудоустройство, проживание и питание людям в трудной жизненной ситуации. Цель анализа — выявить типичные языковые конструкции, тематическую структуру, повторяющиеся клише и характерные приёмы этих объявлений.


## Входные данные

Скрипт принимает табличный файл в одном из форматов: `.xlsx`, `.xls` или `.csv`. Файл должен содержать колонку с текстами объявлений. Допустимые имена колонки: `текст`, `Текст` или `text` — скрипт распознаёт все три варианта автоматически.

Остальные колонки (город, источник, зарплата и т.д.) не обязательны — скрипт работает только с текстовым полем.

Пример структуры входного файла:

| текст | город | источник |
|-------|-------|----------|
| Предлагаем работу с проживанием и питанием... | Москва | вк |
| Рабочий дом приглашает на работу... | Краснодар | авито |


## Зависимости

Для работы нужны только стандартные библиотеки анализа данных:

```
pandas
numpy
scikit-learn
matplotlib
seaborn
```

Специализированные NLP-библиотеки (nltk, natasha, pymystem3, dostoevsky, bertopic) **не требуются**. Стоп-слова, токенизация, извлечение зарплат и разметка клише реализованы встроенными средствами.


## Запуск

### Вариант 1 — как скрипт

Положите файл с данными рядом со скриптом и запустите:

```bash
python nlp_workhouse_analysis.py
```

По умолчанию скрипт ищет файл `test_data.xlsx` в текущей директории. Чтобы указать другой файл, отредактируйте последние строки скрипта:

```python
if __name__ == '__main__':
    analyzer = WorkhouseAnalyzer('ваш_файл.xlsx')  # ← путь к данным
    results = analyzer.run_all(n_topics=6)
    analyzer.save_visualizations('results')
    analyzer.export_data('results')
```

### Вариант 2 — из Jupyter Notebook или другого кода

```python
from nlp_workhouse_analysis import WorkhouseAnalyzer

# Загрузка и полный анализ
analyzer = WorkhouseAnalyzer('data.xlsx')
results = analyzer.run_all(n_topics=6)

# Сохранение графиков и таблиц
analyzer.save_visualizations('results')
analyzer.export_data('results')
```

### Вариант 3 — пошаговый запуск отдельных блоков

```python
from nlp_workhouse_analysis import WorkhouseAnalyzer

analyzer = WorkhouseAnalyzer('data.xlsx')

# Можно вызывать только нужные этапы:
analyzer.frequency_analysis()     # частоты слов, биграмм, TF-IDF
analyzer.topic_modeling(n_topics=6)  # тематическое моделирование
analyzer.domain_analysis()        # доменные маркеры (обещания, риски)
analyzer.cliche_analysis()        # клише и шаблонные фразы
analyzer.structure_analysis()     # структура форматирования

# Результаты доступны в analyzer.results (словарь)
# и в analyzer.df (DataFrame с добавленными колонками)
```


## Что делает каждый блок анализа

### 1. Частотный анализ (`frequency_analysis`)

Подсчитывает частоты отдельных слов, биграмм (пар слов) и триграмм (троек слов) после очистки текста от URL, эмодзи и стоп-слов. Дополнительно рассчитывает TF-IDF — метрику, которая выделяет слова, характерные именно для данного корпуса, а не просто часто встречающиеся.

Результат: таблицы `word_freq`, `bigram_freq`, `trigram_freq`, `tfidf_keywords`.

### 2. Тематическое моделирование (`topic_modeling`)

Применяет алгоритм LDA (Latent Dirichlet Allocation) для автоматического выделения тем в корпусе. Каждому объявлению присваивается основная тема, а каждая тема характеризуется набором ключевых слов.

Параметр `n_topics` задаёт количество тем (по умолчанию 6). К каждому объявлению в DataFrame добавляются колонки `topic` (номер темы), `topic_confidence` (уверенность модели) и `topic_label` (ключевые слова темы).

### 3. Доменный анализ (`domain_analysis`)

Размечает каждое объявление по набору доменных признаков, специфичных для объявлений рабочих домов:

- **Обещания** (бинарные признаки): упоминание проживания, питания, помощи с документами, ежедневной оплаты, свободного выхода.
- **Маркеры давления**: количество маркеров срочности, манипуляции, апелляции к уязвимости.
- **Формальные признаки**: количество восклицательных знаков, доля заглавных букв.
- **Зарплаты**: извлечение числовых значений зарплат из текста с помощью регулярных выражений.

### 4. Анализ клише (`cliche_analysis`)

Ищет в текстах устойчивые шаблонные конструкции, характерные для данного типа объявлений. Поиск ведётся по регулярным выражениям, что позволяет находить фразы в разных словоформах. Примеры отслеживаемых клише:

- «трудная жизненная ситуация»
- «восстановление документов»
- «ежедневная оплата/выплата»
- «трёхразовое питание»
- «свободный выход»
- «бесплатное проживание»
- «с документами и без»

Результат: таблица с названием клише, абсолютной частотой и процентом объявлений.

### 5. Структурный анализ (`structure_analysis`)

Оценивает особенности форматирования текстов: наличие нумерованных списков, буллетов и маркеров, эмодзи, слов, написанных заглавными буквами (КАПСОМ). Это даёт представление о стилистике объявлений.


## Выходные данные

После запуска `save_visualizations()` и `export_data()` в указанной папке (по умолчанию `results/`) появятся следующие файлы.

### Визуализации (PNG)

| Файл | Содержание |
|------|------------|
| `01_top_words.png` | Топ-25 наиболее частых слов |
| `02_top_bigrams.png` | Топ-20 биграмм |
| `03_top_trigrams.png` | Топ-15 триграмм |
| `04_tfidf.png` | Ключевые слова по TF-IDF |
| `05_topics.png` | Распределение по темам LDA + ключевые слова тем |
| `06_cliches.png` | Клише и шаблонные конструкции с частотами |
| `07_offers.png` | Что обещают в объявлениях (проживание, питание и т.д.) |
| `08_scatter.png` | Scatter-plot: обещания vs апелляция к уязвимости |
| `09_lengths.png` | Распределение длины текстов |
| `10_structure.png` | Особенности форматирования |
| `11_salary.png` | Распределение зарплат, извлечённых из текстов |
| `12_dashboard.png` | Сводный дашборд со всеми ключевыми метриками |

### Таблицы (CSV)

| Файл | Содержание |
|------|------------|
| `analyzed_data.csv` | Исходные данные + все вычисленные признаки (тема, маркеры, клише и т.д.) |
| `word_freq.csv` | Частоты слов |
| `bigram_freq.csv` | Частоты биграмм |
| `trigram_freq.csv` | Частоты триграмм |
| `tfidf_keywords.csv` | Ключевые слова по TF-IDF |
| `cliches.csv` | Клише с частотами |


## Пример результатов на тестовых данных

При запуске на тестовом датасете из 232 объявлений (преимущественно из ВКонтакте) были получены следующие основные результаты:

- **84%** объявлений обещают питание, **82%** — проживание, **54%** — помощь с документами, **50%** — ежедневную оплату. При этом «свободный выход» упоминают лишь **1%**.
- Почти половина объявлений (**48%**) содержит конструкцию «трудная жизненная ситуация», **39%** — «восстановление документов», **32%** — «ежедневная оплата/выплата».
- **84%** объявлений используют буллеты и маркеры, **44%** содержат эмодзи — это формат рекламного поста для социальных сетей.
- LDA выделяет 6 тематических кластеров: от общих объявлений рабочих домов до привязанных к конкретным городам и специфичных по типу работ.


## Как расширить анализ

Скрипт легко дополнить новыми блоками. Для этого достаточно добавить метод в класс `WorkhouseAnalyzer` и вызвать его в `run_all()`. Возможные направления расширения:

- **Лемматизация** — если установить pymorphy2 или pymystem3, можно заменить функцию `tokenize()` на версию с лемматизацией для более точного частотного анализа.
- **Сентимент-анализ** — подключение dostoevsky или ruBERT для оценки тональности.
- **Кластеризация** — KMeans или DBSCAN по числовым признакам для группировки объявлений по стилю.
- **Сравнительный анализ по источникам** — если в данных есть колонка с источником, можно сравнить стилистику объявлений с разных площадок.
- **Новые клише** — добавление регулярных выражений в метод `cliche_analysis()` для отслеживания дополнительных шаблонных фраз.
